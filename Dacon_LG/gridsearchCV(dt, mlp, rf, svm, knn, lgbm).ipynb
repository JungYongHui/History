{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import joblib\n",
    "\n",
    "# 필요한 함수 정의\n",
    "def make_datetime(x):\n",
    "    # string 타입의 Time column을 datetime 타입으로 변경\n",
    "    x     = str(x)\n",
    "    year  = int(x[:4])\n",
    "    month = int(x[4:6])\n",
    "    day   = int(x[6:8])\n",
    "    hour  = int(x[8:10])\n",
    "    #mim  = int(x[10:12])\n",
    "    #sec  = int(x[12:])\n",
    "    return dt.datetime(year, month, day, hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt2(X_train, X_test, y_train, y_test):\n",
    "    param_range1 = np.arange(3,15,1)\n",
    "    param_range2 = np.arange(30,60,5)\n",
    "    \n",
    "    param_grid = [{'max_depth':param_range1,\n",
    "                  'min_samples_leaf':param_range2}]\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    \n",
    "    gs = GridSearchCV(estimator=model,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='roc_auc',\n",
    "                     n_jobs=-1)\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gs.predict(X_test)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "\n",
    "    print('DT')\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_params_)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    param_range1 = ['lbfgs', 'adam', 'sgd']\n",
    "    param_range2 = np.arange(1000,2000,100)\n",
    "    param_range3 = np.arange(10, 30, 5)\n",
    "    param_range4 = np.array([0.0001,0.001,0.01])\n",
    "    \n",
    "    param_grid = [{'solver':param_range1,\n",
    "                  'max_iter':param_range2,\n",
    "                  'hidden_layer_sizes':param_range3,\n",
    "                  'learning_rate_init':param_range4}]\n",
    "    \n",
    "    model = MLPClassifier()\n",
    "    \n",
    "    gs = GridSearchCV(estimator=model,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='roc_auc',\n",
    "                     n_jobs=-1)\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "\n",
    "    print('MLP')\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_params_)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    param_range1 = np.arange(3,15,1)\n",
    "    param_range2 = np.arange(30,60,5)\n",
    "    param_range3 = np.arange(30,70,5)\n",
    "    param_range4 = np.array([10, 50, 100])\n",
    "    \n",
    "    param_grid = [{'max_depth':param_range1,\n",
    "                  'min_samples_leaf':param_range2,\n",
    "                  'min_samples_split':param_range3,\n",
    "                  'n_estimators':param_range4}]\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    \n",
    "    gs = GridSearchCV(estimator=model,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='roc_auc',\n",
    "                     n_jobs=-1)\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gs.predict(X_test)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "    \n",
    "    print('RF')\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_params_)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'gamma': [0.001, 0.01, 0.1, 1, 10, 100] }\n",
    "    model = SVC()\n",
    "    \n",
    "    gs = GridSearchCV(estimator=model,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='roc_auc',\n",
    "                     n_jobs=-1)\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gs.predict(X_test)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "    \n",
    "    print('SVM')\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_params_)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    param_grid = {'n_neighbors':[3,5,11,19],\n",
    "                 'weights':['uniform','distance'],\n",
    "                 'metric':['euclidean','manhattan']}\n",
    "    model = KNeighborsClassifier()\n",
    "    gs = GridSearchCV(estimator=model,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='roc_auc',\n",
    "                     n_jobs=-1)\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gs.predict(X_test)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "    \n",
    "    print('KNN')\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_params_)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    param_grid = {\n",
    "        'learning_rate': [0.005, 0.01],\n",
    "        'n_estimators': [8,16,24],\n",
    "        'num_leaves': [6,8,12,16], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "        'boosting_type' : ['gbdt', 'dart'], # for better accuracy -> try dart\n",
    "        'objective' : ['binary'],\n",
    "        'max_bin':[255, 510], # large max_bin helps improve accuracy but might slow down training progress\n",
    "        'random_state' : [500],\n",
    "        'colsample_bytree' : [0.64, 0.65, 0.66],\n",
    "        'subsample' : [0.7,0.75],\n",
    "        'reg_alpha' : [1,1.2],\n",
    "        'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "    \n",
    "    model = LGBMClassifier()\n",
    "    gs = GridSearchCV(estimator=model,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='roc_auc',\n",
    "                     n_jobs=-1)\n",
    "\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gs.predict(X_test)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_test, y_pred, multi_class='ovr')\n",
    "    \n",
    "    print('LGBM')\n",
    "    print(gs.best_score_)\n",
    "    print(gs.best_params_)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= 1 =================================\n",
      "DT\n",
      "0.7787053160765492\n",
      "{'max_depth': 7, 'min_samples_leaf': 40}\n",
      "MLP\n",
      "0.7857254052786181\n",
      "{'hidden_layer_sizes': 25, 'learning_rate_init': 0.01, 'max_iter': 1200, 'solver': 'adam'}\n",
      "RF\n",
      "0.8117116522388853\n",
      "{'max_depth': 11, 'min_samples_leaf': 30, 'min_samples_split': 30, 'n_estimators': 100}\n",
      "SVM\n",
      "0.6652030400907522\n",
      "{'C': 0.01, 'gamma': 0.001}\n",
      "KNN\n",
      "0.6775991283087816\n",
      "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'distance'}\n",
      "LGBM\n",
      "0.80084261997264\n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.64, 'learning_rate': 0.005, 'max_bin': 255, 'n_estimators': 16, 'num_leaves': 16, 'objective': 'binary', 'random_state': 500, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.7}\n",
      "dt accuracy:  0.776\n",
      "dt precision:  0.7794990042972434\n",
      "dt recall:  0.6926227376742251\n",
      "dt auc:  0.6926227376742251\n",
      "mlp accuracy:  0.7712\n",
      "mlp precision:  0.7636811508523007\n",
      "mlp recall:  0.6933017243371934\n",
      "mlp auc:  0.6933017243371935\n",
      "rf accuracy:  0.7808\n",
      "rf precision:  0.7934724215449427\n",
      "rf recall:  0.6945347352240945\n",
      "rf auc:  0.6945347352240946\n",
      "svm accuracy:  0.6688\n",
      "svm precision:  0.3344\n",
      "svm recall:  0.5\n",
      "svm auc:  0.5\n",
      "knn accuracy:  0.6906\n",
      "knn precision:  0.6558758240665239\n",
      "knn recall:  0.561411454360539\n",
      "knn auc:  0.561411454360539\n",
      "lgbm accuracy:  0.6688\n",
      "lgbm precision:  0.3344\n",
      "lgbm recall:  0.5\n",
      "lgbm auc:  0.5\n",
      "========================= 2 =================================\n",
      "DT\n",
      "0.78318106847048\n",
      "{'max_depth': 9, 'min_samples_leaf': 35}\n",
      "MLP\n",
      "0.7813796439476818\n",
      "{'hidden_layer_sizes': 20, 'learning_rate_init': 0.01, 'max_iter': 1500, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    pd.options.display.max_columns=None\n",
    "    \n",
    "    dataframe = pd.read_csv('trainset.csv')\n",
    "    dataframe.index = np.arange(10000, 25000)\n",
    "    dataframe = dataframe.drop(['label'], axis=1)\n",
    "    train_prob = pd.read_csv('train_problem_data.csv')\n",
    "    problem = np.zeros(15000)\n",
    "    problem[train_prob.user_id.unique()-10000] = 1 \n",
    "    \n",
    "    X = dataframe.astype(float).values\n",
    "    y = problem\n",
    "    \n",
    "    kf = KFold(n_splits=3)\n",
    "    i=1\n",
    "    \n",
    "    acc_list, precision_list, recall_list, auc_list = [], [], [], []\n",
    "    acc_list2, precision_list2, recall_list2, auc_list2 = [], [], [], []\n",
    "    acc_list3, precision_list3, recall_list3, auc_list3 = [], [], [], []\n",
    "    acc_list4, precision_list4, recall_list4, auc_list4 = [], [], [], []\n",
    "    acc_list5, precision_list5, recall_list5, auc_list5 = [], [], [], []\n",
    "    acc_list6, precision_list6, recall_list6, auc_list6 = [], [], [], []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print('=========================', i, '=================================')\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        metric1, acc1, precision1, recall1, auc1, model = dt2(X_train, X_test, y_train, y_test)\n",
    "        metric2, acc2, precision2, recall2, auc2, model2 = mlp(X_train, X_test, y_train, y_test)\n",
    "        metric3, acc3, precision3, recall3, auc3, model3 = rf(X_train, X_test, y_train, y_test)\n",
    "        metric4, acc4, precision4, recall4, auc4, model4 = svm(X_train, X_test, y_train, y_test)\n",
    "        metric5, acc5, precision5, recall5, auc5, model5 = knn(X_train, X_test, y_train, y_test)\n",
    "        metric6, acc6, precision6, recall6, auc6, model6 = lgbm(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        \n",
    "        # dt\n",
    "        print('dt accuracy: ', acc1)\n",
    "        print('dt precision: ', precision1)\n",
    "        print('dt recall: ', recall1)\n",
    "        print('dt auc: ', auc1)\n",
    "        \n",
    "        acc_list.append(acc1)\n",
    "        precision_list.append(precision1)\n",
    "        recall_list.append(recall1)\n",
    "        auc_list.append(auc1)\n",
    "        \n",
    "        # mlp\n",
    "        print('mlp accuracy: ', acc2)\n",
    "        print('mlp precision: ', precision2)\n",
    "        print('mlp recall: ', recall2)\n",
    "        print('mlp auc: ', auc2)\n",
    "        \n",
    "        acc_list2.append(acc2)\n",
    "        precision_list2.append(precision2)\n",
    "        recall_list2.append(recall2)\n",
    "        auc_list2.append(auc2)\n",
    "        \n",
    "        # rf\n",
    "        print('rf accuracy: ', acc3)\n",
    "        print('rf precision: ', precision3)\n",
    "        print('rf recall: ', recall3)\n",
    "        print('rf auc: ', auc3)\n",
    "        \n",
    "        acc_list3.append(acc3)\n",
    "        precision_list3.append(precision3)\n",
    "        recall_list3.append(recall3)\n",
    "        auc_list3.append(auc3)\n",
    "        \n",
    "        # svm\n",
    "        print('svm accuracy: ', acc4)\n",
    "        print('svm precision: ', precision4)\n",
    "        print('svm recall: ', recall4)\n",
    "        print('svm auc: ', auc4)\n",
    "        \n",
    "        acc_list4.append(acc4)\n",
    "        precision_list4.append(precision4)\n",
    "        recall_list4.append(recall4)\n",
    "        auc_list4.append(auc4)\n",
    "        \n",
    "        # knn\n",
    "        print('knn accuracy: ', acc5)\n",
    "        print('knn precision: ', precision5)\n",
    "        print('knn recall: ', recall5)\n",
    "        print('knn auc: ', auc5)\n",
    "        \n",
    "        acc_list5.append(acc5)\n",
    "        precision_list5.append(precision5)\n",
    "        recall_list5.append(recall5)\n",
    "        auc_list5.append(auc5)\n",
    "        \n",
    "        # lgbm\n",
    "        print('lgbm accuracy: ', acc6)\n",
    "        print('lgbm precision: ', precision6)\n",
    "        print('lgbm recall: ', recall6)\n",
    "        print('lgbm auc: ', auc6)\n",
    "        \n",
    "        acc_list6.append(acc6)\n",
    "        precision_list6.append(precision6)\n",
    "        recall_list6.append(recall6)\n",
    "        auc_list6.append(auc6)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    print('----------------------- final result ------------------------------')\n",
    "    print('dt average of accuracy', np.mean(acc_list))\n",
    "    print('dt average of precision', np.mean(precision_list))\n",
    "    print('dt average of recall', np.mean(recall_list))\n",
    "    print('dt average of AUC', np.mean(auc_list))\n",
    "    print()\n",
    "    print('mlp average of accuracy', np.mean(acc_list2))\n",
    "    print('mlp average of precision', np.mean(precision_list2))\n",
    "    print('mlp average of recall', np.mean(recall_list2))\n",
    "    print('mlp average of AUC', np.mean(auc_list2))\n",
    "    print()\n",
    "    print('rf average of accuracy', np.mean(acc_list3))\n",
    "    print('rf average of precision', np.mean(precision_list3))\n",
    "    print('rf average of recall', np.mean(recall_list3))\n",
    "    print('rf average of AUC', np.mean(auc_list3))\n",
    "    print()\n",
    "    print('svm average of accuracy', np.mean(acc_list4))\n",
    "    print('svm average of precision', np.mean(precision_list4))\n",
    "    print('svm average of recall', np.mean(recall_list4))\n",
    "    print('svm average of AUC', np.mean(auc_list4))\n",
    "    print()\n",
    "    print('knn average of accuracy', np.mean(acc_list5))\n",
    "    print('knn average of precision', np.mean(precision_list5))\n",
    "    print('knn average of recall', np.mean(recall_list5))\n",
    "    print('knn average of AUC', np.mean(auc_list5))\n",
    "    print()\n",
    "    print('lgbm average of accuracy', np.mean(acc_list6))\n",
    "    print('lgbm average of precision', np.mean(precision_list6))\n",
    "    print('lgbm average of recall', np.mean(recall_list6))\n",
    "    print('lgbm average of AUC', np.mean(auc_list6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.713587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
