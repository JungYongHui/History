{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\YH\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "# 필요한 함수 정의\n",
    "def make_datetime(x):\n",
    "    # string 타입의 Time column을 datetime 타입으로 변경\n",
    "    x     = str(x)\n",
    "    year  = int(x[:4])\n",
    "    month = int(x[4:6])\n",
    "    day   = int(x[6:8])\n",
    "    hour  = int(x[8:10])\n",
    "    #mim  = int(x[10:12])\n",
    "    #sec  = int(x[12:])\n",
    "    return dt.datetime(year, month, day, hour)\n",
    "\n",
    "def make_datetime2(x):\n",
    "    # string 타입의 Time column을 datetime 타입으로 변경\n",
    "    x     = str(x)\n",
    "    year  = int(x[:4])\n",
    "    month = int(x[4:6])\n",
    "    day   = int(x[6:8])\n",
    "    #hour  = int(x[8:10])\n",
    "    #mim  = int(x[10:12])\n",
    "    #sec  = int(x[12:])\n",
    "    return dt.datetime(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_pr_auc(probas_pred, y_true):\n",
    "    labels=y_true.get_label()\n",
    "    p, r, _ = metrics.precision_recall_curve(labels, probas_pred)\n",
    "    score=metrics.auc(r,p) \n",
    "    return \"pr_auc\", score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt2(X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeClassifier(random_state=0,\n",
    "                                  max_depth=9,\n",
    "                                  min_samples_leaf=35)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    y_prob = y_prob[:, 1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    filename = 'dt.model'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train, X_test, y_train, y_test):\n",
    "    model = MLPClassifier(random_state=1,\n",
    "                         hidden_layer_sizes=25,\n",
    "                         learning_rate_init=0.01,\n",
    "                         max_iter=1200,\n",
    "                         solver='adam')\n",
    "    \n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    y_prob = y_prob[:, 1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    filename = 'mlp.model'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestClassifier(random_state=0,\n",
    "                                  max_depth=14,\n",
    "                                  min_samples_leaf=30,\n",
    "                                  min_samples_split=30,\n",
    "                                  n_estimators=100)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    y_prob = y_prob[:, 1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    filename = 'rf.model'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    model = LGBMClassifier(\n",
    "        objective = 'binary',\n",
    "        boosting_type='gbdt',\n",
    "        learning_rate=0.005,\n",
    "        max_bin=255,\n",
    "        n_estimators=16,\n",
    "        num_leaves=16,\n",
    "        reg_alpha=1,\n",
    "        reg_lambda=1,\n",
    "        subsample=0.7\n",
    "    )\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    y_prob = y_prob[:, 1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    filename = 'lgbm.model'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def lgbm2(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    params =      {\n",
    "                    'boosting_type' : 'gbdt',\n",
    "                    'objective'     : 'binary',\n",
    "                    'metric'        : 'auc',\n",
    "                    'seed': 1015\n",
    "                    }\n",
    "    \n",
    "    d_train = lgb.Dataset(X_train, y_train)\n",
    "    d_val = lgb.Dataset(X_test, y_test)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set       = d_train,\n",
    "        num_boost_round = 1000,\n",
    "        valid_sets      = d_val,\n",
    "        feval           = f_pr_auc,\n",
    "        verbose_eval    = 20, \n",
    "        early_stopping_rounds = 3\n",
    "    )\n",
    "\n",
    "    y_prob = model.predict(X_test)\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    filename = 'lgbm2.model'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    return acc, precision, recall, auc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        booster='gbtree',\n",
    "        gamma=3,\n",
    "        max_depth=3,\n",
    "        n_estimators=30\n",
    "    )\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    y_prob = y_prob[:, 1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    filename = 'xgboost.model'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GB(X_train, X_test, y_train, y_test):\n",
    "    model = GradientBoostingClassifier(\n",
    "                   criterion='friedman_mse',\n",
    "                   learning_rate=0.1,\n",
    "                   loss='exponential',\n",
    "                   max_depth=8,\n",
    "                   max_features='log2',\n",
    "                   n_estimators=40)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    y_prob = y_prob[:, 1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    filename = 'GB.model'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost(X_train, X_test, y_train, y_test):\n",
    "    model = CatBoostClassifier(\n",
    "                   depth=4,\n",
    "                   eval_metric='AUC',\n",
    "                   iterations=500,\n",
    "                   leaf_estimation_iterations=10,\n",
    "                   logging_level='Silent',\n",
    "                   loss_function='Logloss',\n",
    "                   random_seed=42)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    y_prob = y_prob[:, 1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    metric = metrics.confusion_matrix(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    filename = 'catboost.model'\n",
    "    joblib.dump(model, filename)\n",
    "    \n",
    "    return metric, acc, precision, recall, auc, f1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= 1 =================================\n",
      "[LightGBM] [Info] Number of positive: 4024, number of negative: 7976\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 53521\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 513\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.335333 -> initscore=-0.684161\n",
      "[LightGBM] [Info] Start training from score -0.684161\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's auc: 0.80627\tvalid_0's pr_auc: 0.726087\n",
      "[16:52:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "dt accuracy:  0.7746666666666666\n",
      "dt precision:  0.7329192546583851\n",
      "dt recall:  0.48360655737704916\n",
      "dt f1:  0.582716049382716\n",
      "dt auc:  0.7746946984546104\n",
      "\n",
      "mlp accuracy:  0.73\n",
      "mlp precision:  0.7643312101910829\n",
      "mlp recall:  0.2459016393442623\n",
      "mlp f1:  0.37209302325581395\n",
      "mlp auc:  0.7491500558867361\n",
      "\n",
      "rf accuracy:  0.7873333333333333\n",
      "rf precision:  0.8039568345323741\n",
      "rf recall:  0.45799180327868855\n",
      "rf f1:  0.5835509138381201\n",
      "rf auc:  0.811929995788246\n",
      "\n",
      "lgbm accuracy:  0.6746666666666666\n",
      "lgbm precision:  0.0\n",
      "lgbm recall:  0.0\n",
      "lgbm f1:  0.0\n",
      "lgbm auc:  0.782885345120845\n",
      "\n",
      "lgbm2 accuracy:  0.785\n",
      "lgbm2 precision:  0.8839907192575406\n",
      "lgbm2 recall:  0.39036885245901637\n",
      "lgbm2 f1:  0.5415778251599147\n",
      "lgbm2 auc:  0.8062704513056437\n",
      "\n",
      "xgboost accuracy:  0.7976666666666666\n",
      "xgboost precision:  0.7923930269413629\n",
      "xgboost recall:  0.5122950819672131\n",
      "xgboost f1:  0.6222775357809583\n",
      "xgboost auc:  0.8153768507419167\n",
      "\n",
      "GB accuracy:  0.7926666666666666\n",
      "GB precision:  0.7940199335548173\n",
      "GB recall:  0.48975409836065575\n",
      "GB f1:  0.605830164765526\n",
      "GB auc:  0.814758249368237\n",
      "\n",
      "catboost accuracy:  0.803\n",
      "catboost precision:  0.8202995008319468\n",
      "catboost recall:  0.5051229508196722\n",
      "catboost f1:  0.6252377932783768\n",
      "catboost auc:  0.821091573252122\n",
      "========================= 2 =================================\n",
      "[LightGBM] [Info] Number of positive: 3987, number of negative: 8013\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 53592\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 513\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.332250 -> initscore=-0.698026\n",
      "[LightGBM] [Info] Start training from score -0.698026\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "[20]\tvalid_0's auc: 0.827372\tvalid_0's pr_auc: 0.749638\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's auc: 0.830178\tvalid_0's pr_auc: 0.75431\n",
      "[16:53:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "dt accuracy:  0.7853333333333333\n",
      "dt precision:  0.7808219178082192\n",
      "dt recall:  0.506416584402764\n",
      "dt f1:  0.6143712574850299\n",
      "dt auc:  0.7878257041947385\n",
      "\n",
      "mlp accuracy:  0.6993333333333334\n",
      "mlp precision:  0.5508707607699358\n",
      "mlp recall:  0.5932872655478776\n",
      "mlp f1:  0.5712927756653993\n",
      "mlp auc:  0.7260813252578086\n",
      "\n",
      "rf accuracy:  0.7816666666666666\n",
      "rf precision:  0.8023648648648649\n",
      "rf recall:  0.46890424481737414\n",
      "rf f1:  0.5919003115264797\n",
      "rf auc:  0.8190618089645877\n",
      "\n",
      "lgbm accuracy:  0.6623333333333333\n",
      "lgbm precision:  0.0\n",
      "lgbm recall:  0.0\n",
      "lgbm f1:  0.0\n",
      "lgbm auc:  0.8027355500784716\n",
      "\n",
      "lgbm2 accuracy:  0.792\n",
      "lgbm2 precision:  0.81421647819063\n",
      "lgbm2 recall:  0.49753208292201384\n",
      "lgbm2 f1:  0.6176470588235294\n",
      "lgbm2 auc:  0.8301784898980591\n",
      "\n",
      "xgboost accuracy:  0.7873333333333333\n",
      "xgboost precision:  0.7971473851030111\n",
      "xgboost recall:  0.49654491609081935\n",
      "xgboost f1:  0.6119221411192215\n",
      "xgboost auc:  0.8289814197019024\n",
      "\n",
      "GB accuracy:  0.7853333333333333\n",
      "GB precision:  0.7914691943127962\n",
      "GB recall:  0.4945705824284304\n",
      "GB f1:  0.6087484811664641\n",
      "GB auc:  0.8239742929237477\n",
      "\n",
      "catboost accuracy:  0.789\n",
      "catboost precision:  0.7987421383647799\n",
      "catboost recall:  0.5014807502467917\n",
      "catboost f1:  0.6161309884778654\n",
      "catboost auc:  0.8317464307733734\n",
      "========================= 3 =================================\n",
      "[LightGBM] [Info] Number of positive: 4026, number of negative: 7974\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 53591\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 513\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.335500 -> initscore=-0.683413\n",
      "[LightGBM] [Info] Start training from score -0.683413\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's auc: 0.830934\tvalid_0's pr_auc: 0.748095\n",
      "[16:54:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "dt accuracy:  0.7906666666666666\n",
      "dt precision:  0.751453488372093\n",
      "dt recall:  0.5308008213552361\n",
      "dt f1:  0.622141997593261\n",
      "dt auc:  0.7903610861672995\n",
      "\n",
      "mlp accuracy:  0.328\n",
      "mlp precision:  0.32528504359490273\n",
      "mlp recall:  0.9958932238193019\n",
      "mlp f1:  0.4903943377148634\n",
      "mlp auc:  0.501644940212555\n",
      "\n",
      "rf accuracy:  0.796\n",
      "rf precision:  0.7986798679867987\n",
      "rf recall:  0.49691991786447637\n",
      "rf f1:  0.6126582278481012\n",
      "rf auc:  0.8256028913650267\n",
      "\n",
      "lgbm accuracy:  0.6753333333333333\n",
      "lgbm precision:  0.0\n",
      "lgbm recall:  0.0\n",
      "lgbm f1:  0.0\n",
      "lgbm auc:  0.8036414699258714\n",
      "\n",
      "lgbm2 accuracy:  0.8033333333333333\n",
      "lgbm2 precision:  0.8529411764705882\n",
      "lgbm2 recall:  0.47638603696098564\n",
      "lgbm2 f1:  0.61133069828722\n",
      "lgbm2 auc:  0.8309342510403765\n",
      "\n",
      "xgboost accuracy:  0.808\n",
      "xgboost precision:  0.8024316109422492\n",
      "xgboost recall:  0.5420944558521561\n",
      "xgboost f1:  0.6470588235294119\n",
      "xgboost auc:  0.8371167127141818\n",
      "\n",
      "GB accuracy:  0.8023333333333333\n",
      "GB precision:  0.7873303167420814\n",
      "GB recall:  0.5359342915811088\n",
      "GB f1:  0.6377519853390349\n",
      "GB auc:  0.8303912586073042\n",
      "\n",
      "catboost accuracy:  0.811\n",
      "catboost precision:  0.8050974512743628\n",
      "catboost recall:  0.5513347022587269\n",
      "catboost f1:  0.6544789762340038\n",
      "catboost auc:  0.838506499692904\n",
      "========================= 4 =================================\n",
      "[LightGBM] [Info] Number of positive: 3979, number of negative: 8021\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 53593\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 513\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.331583 -> initscore=-0.701033\n",
      "[LightGBM] [Info] Start training from score -0.701033\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.81688\tvalid_0's pr_auc: 0.741975\n",
      "[16:54:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "dt accuracy:  0.7823333333333333\n",
      "dt precision:  0.7636103151862464\n",
      "dt recall:  0.5220372184133203\n",
      "dt f1:  0.620127981384526\n",
      "dt auc:  0.789585208845671\n",
      "\n",
      "mlp accuracy:  0.3416666666666667\n",
      "mlp precision:  0.33993288590604026\n",
      "mlp recall:  0.9921645445641528\n",
      "mlp f1:  0.5063734066483379\n",
      "mlp auc:  0.4993667593967809\n",
      "\n",
      "rf accuracy:  0.7836666666666666\n",
      "rf precision:  0.8206896551724138\n",
      "rf recall:  0.46620959843290893\n",
      "rf f1:  0.594628357276702\n",
      "rf auc:  0.81870858509947\n",
      "\n",
      "lgbm accuracy:  0.6596666666666666\n",
      "lgbm precision:  0.0\n",
      "lgbm recall:  0.0\n",
      "lgbm f1:  0.0\n",
      "lgbm auc:  0.7905851796458307\n",
      "\n",
      "lgbm2 accuracy:  0.7876666666666666\n",
      "lgbm2 precision:  0.8568773234200744\n",
      "lgbm2 recall:  0.4515181194906954\n",
      "lgbm2 f1:  0.5914047466324567\n",
      "lgbm2 auc:  0.8168796357839587\n",
      "\n",
      "xgboost accuracy:  0.7983333333333333\n",
      "xgboost precision:  0.8180428134556575\n",
      "xgboost recall:  0.5239960822722821\n",
      "xgboost f1:  0.6388059701492538\n",
      "xgboost auc:  0.8239497089666771\n",
      "\n",
      "GB accuracy:  0.79\n",
      "GB precision:  0.7948717948717948\n",
      "GB recall:  0.5161606268364348\n",
      "GB f1:  0.6258907363420427\n",
      "GB auc:  0.8191228269008726\n",
      "\n",
      "catboost accuracy:  0.7993333333333333\n",
      "catboost precision:  0.8188736681887366\n",
      "catboost recall:  0.5269343780607247\n",
      "catboost f1:  0.6412395709177592\n",
      "catboost auc:  0.8309215420089193\n",
      "========================= 5 =================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3984, number of negative: 8016\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 53588\n",
      "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 513\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.332000 -> initscore=-0.699153\n",
      "[LightGBM] [Info] Start training from score -0.699153\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's auc: 0.83873\tvalid_0's pr_auc: 0.777246\n",
      "[16:55:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "dt accuracy:  0.779\n",
      "dt precision:  0.7678300455235205\n",
      "dt recall:  0.49803149606299213\n",
      "dt f1:  0.6041791044776119\n",
      "dt auc:  0.8000839888398527\n",
      "\n",
      "mlp accuracy:  0.7356666666666667\n",
      "mlp precision:  0.6632503660322109\n",
      "mlp recall:  0.44586614173228345\n",
      "mlp f1:  0.5332548557975281\n",
      "mlp auc:  0.7726769867602236\n",
      "\n",
      "rf accuracy:  0.7943333333333333\n",
      "rf precision:  0.8387096774193549\n",
      "rf recall:  0.4862204724409449\n",
      "rf f1:  0.6155763239875389\n",
      "rf auc:  0.8346714662179324\n",
      "\n",
      "lgbm accuracy:  0.6613333333333333\n",
      "lgbm precision:  0.0\n",
      "lgbm recall:  0.0\n",
      "lgbm f1:  0.0\n",
      "lgbm auc:  0.8186850611982475\n",
      "\n",
      "lgbm2 accuracy:  0.801\n",
      "lgbm2 precision:  0.8694885361552028\n",
      "lgbm2 recall:  0.48523622047244097\n",
      "lgbm2 f1:  0.6228679722046747\n",
      "lgbm2 auc:  0.8387300173037846\n",
      "\n",
      "xgboost accuracy:  0.8083333333333333\n",
      "xgboost precision:  0.832579185520362\n",
      "xgboost recall:  0.5433070866141733\n",
      "xgboost f1:  0.6575342465753424\n",
      "xgboost auc:  0.8430686138716028\n",
      "\n",
      "GB accuracy:  0.8016666666666666\n",
      "GB precision:  0.8263565891472868\n",
      "GB recall:  0.5246062992125984\n",
      "GB f1:  0.641782059000602\n",
      "GB auc:  0.8429259866332233\n",
      "\n",
      "catboost accuracy:  0.8103333333333333\n",
      "catboost precision:  0.8443759630200308\n",
      "catboost recall:  0.5393700787401575\n",
      "catboost f1:  0.6582582582582583\n",
      "catboost auc:  0.8523899860299722\n",
      "----------------------- final result ------------------------------\n",
      "dt average of accuracy 0.7824\n",
      "dt average of precsion 0.7593270043096928\n",
      "dt average of recall 0.5081785355222723\n",
      "dt average of f1 0.608707278064629\n",
      "dt average of AUC 0.7885101373004344\n",
      "\n",
      "mlp average of accuracy 0.5669333333333334\n",
      "mlp average of precsion 0.5287340532988345\n",
      "mlp average of recall 0.6546225630015756\n",
      "mlp average of f1 0.49468167981638855\n",
      "mlp average of AUC 0.6497840135028208\n",
      "\n",
      "rf average of accuracy 0.7886\n",
      "rf average of precsion 0.8128801799951614\n",
      "rf average of recall 0.4752492073668785\n",
      "rf average of f1 0.5996628268953883\n",
      "rf average of AUC 0.8219949494870527\n",
      "\n",
      "lgbm average of accuracy 0.6666666666666666\n",
      "lgbm average of precsion 0.0\n",
      "lgbm average of recall 0.0\n",
      "lgbm average of f1 0.0\n",
      "lgbm average of AUC 0.7997065211938532\n",
      "\n",
      "lgbm2 average of accuracy 0.7938\n",
      "lgbm2 average of precsion 0.8555028466988072\n",
      "lgbm2 average of recall 0.4602082624610304\n",
      "lgbm2 average of f1 0.5969656602215592\n",
      "lgbm2 average of AUC 0.8245985690663644\n",
      "\n",
      "xgboost average of accuracy 0.7999333333333333\n",
      "xgboost average of precsion 0.8085188043925285\n",
      "xgboost average of recall 0.5236475245593287\n",
      "xgboost average of f1 0.6355197434308375\n",
      "xgboost average of AUC 0.8296986611992562\n",
      "\n",
      "GB average of accuracy 0.7944\n",
      "GB average of precsion 0.7988095657257552\n",
      "GB average of recall 0.5122051796838457\n",
      "GB average of f1 0.6240006853227339\n",
      "GB average of AUC 0.8262345228866769\n",
      "\n",
      "catboost average of accuracy 0.8025333333333332\n",
      "catboost average of precsion 0.8174777443359714\n",
      "catboost average of recall 0.5248485720252146\n",
      "catboost average of f1 0.6390691174332528\n",
      "catboost average of AUC 0.8349312063514581\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    pd.options.display.max_columns=None\n",
    "    \n",
    "    dataframe = pd.read_csv('train0127.csv')\n",
    "    dataframe.index = np.arange(10000, 25000)\n",
    "    dataframe2 = pd.read_csv('test0127.csv')\n",
    "    dataframe2.index = np.arange(30000, 44999)\n",
    "    dataframe2 = dataframe2.fillna(0)\n",
    "    \n",
    "    train_prob = pd.read_csv('train_problem_data.csv')\n",
    "    problem = np.zeros(15000)\n",
    "    problem[train_prob.user_id.unique()-10000] = 1 \n",
    "    \n",
    "    X = dataframe.astype(float).values\n",
    "    y = problem\n",
    "    X_te = dataframe2.astype(float).values\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    i=1\n",
    "    \n",
    "    acc_list, precision_list, recall_list, f1_list, auc_list = [], [], [], [], []\n",
    "    acc_list2, precision_list2, recall_list2, f1_list2, auc_list2 = [], [], [], [], []\n",
    "    acc_list3, precision_list3, recall_list3, f1_list3, auc_list3 = [], [], [], [], []\n",
    "    acc_list4, precision_list4, recall_list4, f1_list4, auc_list4 = [], [], [], [], []\n",
    "    acc_list5, precision_list5, recall_list5, f1_list5, auc_list5 = [], [], [], [], []\n",
    "    acc_list6, precision_list6, recall_list6, f1_list6, auc_list6 = [], [], [], [], []\n",
    "    acc_list7, precision_list7, recall_list7, f1_list7, auc_list7 = [], [], [], [], []\n",
    "    acc_list8, precision_list8, recall_list8, f1_list8, auc_list8 = [], [], [], [], []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print('=========================', i, '=================================')\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        metric1, acc1, precision1, recall1, auc1, f11, model = dt2(X_train, X_test, y_train, y_test)\n",
    "        metric2, acc2, precision2, recall2, auc2, f12, model2 = mlp(X_train, X_test, y_train, y_test)\n",
    "        metric3, acc3, precision3, recall3, auc3, f13, model3 = rf(X_train, X_test, y_train, y_test)\n",
    "        metric4, acc4, precision4, recall4, auc4, f14, model4 = lgbm(X_train, X_test, y_train, y_test)\n",
    "        acc5, precision5, recall5, auc5, f15, model5 = lgbm2(X_train, X_test, y_train, y_test)\n",
    "        metric6, acc6, precision6, recall6, auc6, f16, model6 = xgboost(X_train, X_test, y_train, y_test)\n",
    "        metric7, acc7, precision7, recall7, auc7, f17, model7 = GB(X_train, X_test, y_train, y_test)\n",
    "        metric8, acc8, precision8, recall8, auc8, f18, model8 = catboost(X_train, X_test, y_train, y_test)\n",
    "\n",
    "        \n",
    "        # dt\n",
    "        print('dt accuracy: ', acc1)\n",
    "        print('dt precision: ', precision1)\n",
    "        print('dt recall: ', recall1)\n",
    "        print('dt f1: ', f11)\n",
    "        print('dt auc: ', auc1)\n",
    "        print()\n",
    "        acc_list.append(acc1)\n",
    "        precision_list.append(precision1)\n",
    "        recall_list.append(recall1)\n",
    "        f1_list.append(f11)\n",
    "        auc_list.append(auc1)\n",
    "        \n",
    "        # mlp\n",
    "        print('mlp accuracy: ', acc2)\n",
    "        print('mlp precision: ', precision2)\n",
    "        print('mlp recall: ', recall2)\n",
    "        print('mlp f1: ', f12)\n",
    "        print('mlp auc: ', auc2)\n",
    "        print()\n",
    "        acc_list2.append(acc2)\n",
    "        precision_list2.append(precision2)\n",
    "        recall_list2.append(recall2)\n",
    "        f1_list2.append(f12)\n",
    "        auc_list2.append(auc2)\n",
    "        \n",
    "        # rf\n",
    "        print('rf accuracy: ', acc3)\n",
    "        print('rf precision: ', precision3)\n",
    "        print('rf recall: ', recall3)\n",
    "        print('rf f1: ', f13)\n",
    "        print('rf auc: ', auc3)\n",
    "        print()\n",
    "        acc_list3.append(acc3)\n",
    "        precision_list3.append(precision3)\n",
    "        recall_list3.append(recall3)\n",
    "        f1_list3.append(f13)\n",
    "        auc_list3.append(auc3)\n",
    "        \n",
    "        # lgbm\n",
    "        print('lgbm accuracy: ', acc4)\n",
    "        print('lgbm precision: ', precision4)\n",
    "        print('lgbm recall: ', recall4)\n",
    "        print('lgbm f1: ', f14)\n",
    "        print('lgbm auc: ', auc4)\n",
    "        print()\n",
    "        acc_list4.append(acc4)\n",
    "        precision_list4.append(precision4)\n",
    "        recall_list4.append(recall4)\n",
    "        f1_list4.append(f14)\n",
    "        auc_list4.append(auc4)\n",
    "        \n",
    "        # lgbm2\n",
    "        print('lgbm2 accuracy: ', acc5)\n",
    "        print('lgbm2 precision: ', precision5)\n",
    "        print('lgbm2 recall: ', recall5)\n",
    "        print('lgbm2 f1: ', f15)\n",
    "        print('lgbm2 auc: ', auc5)\n",
    "        print()\n",
    "        acc_list5.append(acc5)\n",
    "        precision_list5.append(precision5)\n",
    "        recall_list5.append(recall5)\n",
    "        f1_list5.append(f15)\n",
    "        auc_list5.append(auc5)\n",
    "        \n",
    "        # xgboost\n",
    "        print('xgboost accuracy: ', acc6)\n",
    "        print('xgboost precision: ', precision6)\n",
    "        print('xgboost recall: ', recall6)\n",
    "        print('xgboost f1: ', f16)\n",
    "        print('xgboost auc: ', auc6)\n",
    "        print()\n",
    "        acc_list6.append(acc6)\n",
    "        precision_list6.append(precision6)\n",
    "        recall_list6.append(recall6)\n",
    "        f1_list6.append(f16)\n",
    "        auc_list6.append(auc6)\n",
    "        \n",
    "        # GB\n",
    "        print('GB accuracy: ', acc7)\n",
    "        print('GB precision: ', precision7)\n",
    "        print('GB recall: ', recall7)\n",
    "        print('GB f1: ', f17)\n",
    "        print('GB auc: ', auc7)\n",
    "        print()\n",
    "        acc_list7.append(acc7)\n",
    "        precision_list7.append(precision7)\n",
    "        recall_list7.append(recall7)\n",
    "        f1_list7.append(f17)\n",
    "        auc_list7.append(auc7)\n",
    "        \n",
    "        # catboost\n",
    "        print('catboost accuracy: ', acc8)\n",
    "        print('catboost precision: ', precision8)\n",
    "        print('catboost recall: ', recall8)\n",
    "        print('catboost f1: ', f18)\n",
    "        print('catboost auc: ', auc8)\n",
    "        \n",
    "        acc_list8.append(acc8)\n",
    "        precision_list8.append(precision8)\n",
    "        recall_list8.append(recall8)\n",
    "        f1_list8.append(f18)\n",
    "        auc_list8.append(auc8)\n",
    "        \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    print('----------------------- final result ------------------------------')\n",
    "    print('dt average of accuracy', np.mean(acc_list))\n",
    "    print('dt average of precsion', np.mean(precision_list))\n",
    "    print('dt average of recall', np.mean(recall_list))\n",
    "    print('dt average of f1', np.mean(f1_list))\n",
    "    print('dt average of AUC', np.mean(auc_list))\n",
    "    print()\n",
    "    print('mlp average of accuracy', np.mean(acc_list2))\n",
    "    print('mlp average of precsion', np.mean(precision_list2))\n",
    "    print('mlp average of recall', np.mean(recall_list2))\n",
    "    print('mlp average of f1', np.mean(f1_list2))\n",
    "    print('mlp average of AUC', np.mean(auc_list2))\n",
    "    print()\n",
    "    print('rf average of accuracy', np.mean(acc_list3))\n",
    "    print('rf average of precsion', np.mean(precision_list3))\n",
    "    print('rf average of recall', np.mean(recall_list3))\n",
    "    print('rf average of f1', np.mean(f1_list3))\n",
    "    print('rf average of AUC', np.mean(auc_list3))\n",
    "    print()\n",
    "    print('lgbm average of accuracy', np.mean(acc_list4))\n",
    "    print('lgbm average of precsion', np.mean(precision_list4))\n",
    "    print('lgbm average of recall', np.mean(recall_list4))\n",
    "    print('lgbm average of f1', np.mean(f1_list4))\n",
    "    print('lgbm average of AUC', np.mean(auc_list4))\n",
    "    print()\n",
    "    print('lgbm2 average of accuracy', np.mean(acc_list5))\n",
    "    print('lgbm2 average of precsion', np.mean(precision_list5))\n",
    "    print('lgbm2 average of recall', np.mean(recall_list5))\n",
    "    print('lgbm2 average of f1', np.mean(f1_list5))\n",
    "    print('lgbm2 average of AUC', np.mean(auc_list5))\n",
    "    print()\n",
    "    print('xgboost average of accuracy', np.mean(acc_list6))\n",
    "    print('xgboost average of precsion', np.mean(precision_list6))\n",
    "    print('xgboost average of recall', np.mean(recall_list6))\n",
    "    print('xgboost average of f1', np.mean(f1_list6))\n",
    "    print('xgboost average of AUC', np.mean(auc_list6))\n",
    "    print()\n",
    "    print('GB average of accuracy', np.mean(acc_list7))\n",
    "    print('GB average of precsion', np.mean(precision_list7))\n",
    "    print('GB average of recall', np.mean(recall_list7))\n",
    "    print('GB average of f1', np.mean(f1_list7))\n",
    "    print('GB average of AUC', np.mean(auc_list7))\n",
    "    print()\n",
    "    print('catboost average of accuracy', np.mean(acc_list8))\n",
    "    print('catboost average of precsion', np.mean(precision_list8))\n",
    "    print('catboost average of recall', np.mean(recall_list8))\n",
    "    print('catboost average of f1', np.mean(f1_list8))\n",
    "    print('catboost average of AUC', np.mean(auc_list8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
