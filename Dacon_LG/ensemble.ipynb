{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\YH\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "# 필요한 함수 정의\n",
    "def make_datetime(x):\n",
    "    # string 타입의 Time column을 datetime 타입으로 변경\n",
    "    x     = str(x)\n",
    "    year  = int(x[:4])\n",
    "    month = int(x[4:6])\n",
    "    day   = int(x[6:8])\n",
    "    hour  = int(x[8:10])\n",
    "    #mim  = int(x[10:12])\n",
    "    #sec  = int(x[12:])\n",
    "    return dt.datetime(year, month, day, hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_pr_auc(probas_pred, y_true):\n",
    "    labels=y_true.get_label()\n",
    "    p, r, _ = metrics.precision_recall_curve(labels, probas_pred)\n",
    "    score=metrics.auc(r,p) \n",
    "    return \"pr_auc\", score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psuedo_labeling(model, X_tr, X_val, y_tr, y_val, X_te):\n",
    "    \n",
    "    y_prob = np.round(model.predict_proba(X_te), 2)\n",
    "    y_prob = y_prob[:, 1]\n",
    "    y_pred = np.where(y_prob > 0.5, 1, 0)\n",
    "    y_te = y_pred\n",
    "\n",
    "    new_X = np.concatenate((X_tr, X_te), axis=0)\n",
    "    new_y = np.concatenate((y_tr, y_te), axis=0)\n",
    "    \n",
    "    model = model.fit(new_X, new_y)\n",
    "    \n",
    "    return model, new_X, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smote 알고리즘으로 데이터 불균형 해결 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from imblearn.over_sampling import SMOTE\\n\\ndef smote(X_train, y_train):\\n    smote = SMOTE(random_state=0)\\n    X_train_over, y_train_over = smote.fit_sample(X_train, y_train)\\n    print('smote 적용 전 데이터 세트: ', X_train.shape, y_train.shape)\\n    print('smote 적용 후 데이터 세트: ', X_train_over.shape, y_train_over.shape)\\n    print('smote 적용 후 label 분포: \\n', pd.Series(y_train_over).value_counts())\\n    return X_train_over, y_train_over\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def smote(X_train, y_train):\n",
    "    smote = SMOTE(random_state=0)\n",
    "    X_train_over, y_train_over = smote.fit_sample(X_train, y_train)\n",
    "    print('smote 적용 전 데이터 세트: ', X_train.shape, y_train.shape)\n",
    "    print('smote 적용 후 데이터 세트: ', X_train_over.shape, y_train_over.shape)\n",
    "    print('smote 적용 후 label 분포: \\n', pd.Series(y_train_over).value_counts())\n",
    "    return X_train_over, y_train_over\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve plotting 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "def plot_roc(model, i):\n",
    "    fig = plt.figure(figsize=(18, 3))\n",
    "    ax = fig.add_subplot(1,5,i)\n",
    "    visualizer = ROCAUC(model, classes=['non_problem', 'problem'], micro=False, per_class=True)\n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_valid, y_valid)\n",
    "    visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기존 데이터로만 학습한 모델들의 앙상블 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= 1 =================================\n",
      "[17:20:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    pd.options.display.max_columns=None\n",
    "    \n",
    "    dataframe = pd.read_csv('train0127.csv')\n",
    "    dataframe.index = np.arange(10000, 25000)\n",
    "    dataframe2 = pd.read_csv('test0127.csv')\n",
    "    dataframe2.index = np.arange(30000, 44999)\n",
    "    dataframe2 = dataframe2.fillna(0)\n",
    "\n",
    "    train_prob = pd.read_csv('train_problem_data.csv')\n",
    "    problem = np.zeros(15000)\n",
    "    problem[train_prob.user_id.unique()-10000] = 1 \n",
    "    \n",
    "    X = dataframe.astype(float).values\n",
    "    y = problem\n",
    "    X_te = dataframe2.astype(float).values\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    i=1\n",
    "    \n",
    "    acc_list, precision_list, recall_list, f1_list, auc_list = [], [], [], [], []\n",
    "    acc_list2, precision_list2, recall_list2, f1_list2, auc_list2 = [], [], [], [], []\n",
    "    \n",
    "    model_list=[]\n",
    "    for train_index, valid_index in kf.split(X):\n",
    "        print('=========================', i, '=================================')\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        \n",
    "        \n",
    "        xgboost_model = joblib.load('xgboost.model')\n",
    "\n",
    "        rf_model = joblib.load('rf.model')\n",
    "\n",
    "        GB_model = joblib.load('GB.model')\n",
    "        \n",
    "        CB_model = joblib.load('catboost.model')\n",
    "\n",
    "            \n",
    "        \n",
    "        models = [\n",
    "            ('XGBOOST', xgboost_model),\n",
    "            #('RF', rf_model),\n",
    "            ('GB', GB_model),\n",
    "            ('CatBoost', CB_model)]\n",
    "\n",
    "\n",
    "        # soft vote\n",
    "        soft_vote  = VotingClassifier(models, voting='soft')\n",
    "        model2 = soft_vote.fit(X_train, y_train)\n",
    "        model_list.append(model2)\n",
    "        \n",
    "        y_prob2 = np.round(model2.predict_proba(X_valid), 2)\n",
    "        y_prob2 = y_prob2[:, 1]\n",
    "        y_pred2 = np.where(y_prob2 > 0.5, 1, 0)\n",
    "        \n",
    "        metric2 = metrics.confusion_matrix(y_valid, y_pred2)\n",
    "        acc2 = metrics.accuracy_score(y_valid, y_pred2)\n",
    "        precision2 = metrics.precision_score(y_valid, y_pred2)\n",
    "        recall2 = metrics.recall_score(y_valid, y_pred2)\n",
    "        f12 = metrics.f1_score(y_valid, y_pred2)\n",
    "        auc2 = metrics.roc_auc_score(y_valid, y_prob2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # softvote\n",
    "        print('softvote accuracy: ', acc2)\n",
    "        print('softvote precision: ', precision2)\n",
    "        print('softvote recall: ', recall2)\n",
    "        print('softvote f1: ', f12)\n",
    "        print('softvote auc: ', auc2)\n",
    "        \n",
    "        acc_list2.append(acc2)\n",
    "        precision_list2.append(precision2)\n",
    "        recall_list2.append(recall2)\n",
    "        f1_list2.append(f12)\n",
    "        auc_list2.append(auc2)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    print('----------------------- final result ------------------------------')\n",
    "    print('softvote average of accuracy', np.mean(acc_list2))\n",
    "    print('softvote average of precision', np.mean(precision_list2))\n",
    "    print('softvote average of recall', np.mean(recall_list2))\n",
    "    print('softvote average of f1', np.mean(f1_list2))\n",
    "    print('softvote average of AUC', np.mean(auc_list2))\n",
    "    \n",
    "    for i in range(len(model_list)):\n",
    "        plot_roc(model_list[i], i+1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_y_list = []\n",
    "for model in model_list:\n",
    "    pred_y = model.predict_proba(X_te)\n",
    "    pred_y = pred_y[:,1]\n",
    "    pred_y_list.append(pred_y.reshape(-1,1))\n",
    "    \n",
    "mean_pred = np.mean(pred_y_list, axis=0)\n",
    "print(mean_pred)\n",
    "print(len(mean_pred))\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv')\n",
    "submission['problem'] = mean_pred.reshape(-1)\n",
    "submission.to_csv('ensemble_xgboost_rf_GB_CB_0129.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(submission.problem)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "tot_1 = 0\n",
    "tot_0 = 0\n",
    "for i in submission.problem:\n",
    "    if i > 0.5:\n",
    "        tot_1 += 1\n",
    "    else:\n",
    "        tot_0 += 1\n",
    "        \n",
    "print('0_class number: ', tot_0)\n",
    "print('1_class number: ', tot_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model2.predict_proba(X_te)\n",
    "pred_y = pred_y[:,1]\n",
    "    \n",
    "mean_pred = pred_y\n",
    "print(mean_pred)\n",
    "print(len(mean_pred))\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv')\n",
    "submission['problem'] = mean_pred.reshape(-1)\n",
    "submission.to_csv('kfx_ensemble_xgboost_rf_GB_CB_0129.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(submission.problem)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "tot_1 = 0\n",
    "tot_0 = 0\n",
    "for i in submission.problem:\n",
    "    if i > 0.5:\n",
    "        tot_1 += 1\n",
    "    else:\n",
    "        tot_0 += 1\n",
    "\n",
    "print('0_class number: ', tot_0)\n",
    "print('1_class number: ', tot_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# psuedo labeling 데이터 학습시킨 모델의 앙상블 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    pd.options.display.max_columns=None\n",
    "    \n",
    "    dataframe = pd.read_csv('train0127.csv')\n",
    "    dataframe.index = np.arange(10000, 25000)\n",
    "\n",
    "    dataframe2 = pd.read_csv('test0127.csv')\n",
    "    dataframe2.index = np.arange(30000, 44999)\n",
    "    dataframe2 = dataframe2.fillna(0)\n",
    "    \n",
    "    train_prob = pd.read_csv('train_problem_data.csv')\n",
    "    problem = np.zeros(15000)\n",
    "    problem[train_prob.user_id.unique()-10000] = 1 \n",
    "    \n",
    "    X = dataframe.astype(float).values\n",
    "    y = problem\n",
    "    # X, y = smote(X, y)\n",
    "    X_te = dataframe2\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    i=1\n",
    "    \n",
    "\n",
    "    acc_list2, precision_list2, recall_list2, auc_list2 = [], [], [], []\n",
    "    \n",
    "    model_list=[]\n",
    "    for train_index, valid_index in kf.split(X):\n",
    "        print('=========================', i, '=================================')\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        \n",
    "        \n",
    "        xgboost_model = joblib.load('xgboost.model')\n",
    "        xgboost_psuedo, new_X, new_y = psuedo_labeling(xgboost_model, X_train, X_valid, y_train, y_valid, X_te)\n",
    "        rf_model = joblib.load('rf.model')\n",
    "        rf_psuedo,_,_ = psuedo_labeling(rf_model, X_train, X_valid, y_train, y_valid, X_te)\n",
    "        GB_model = joblib.load('GB.model')\n",
    "        GB_psuedo,_,_ = psuedo_labeling(GB_model, X_train, X_valid, y_train, y_valid, X_te)\n",
    "        CB_model = joblib.load('catboost.model')\n",
    "        CB_psuedo,_,_ = psuedo_labeling(CB_model, X_train, X_valid, y_train, y_valid, X_te)    \n",
    "        \n",
    "        models = [\n",
    "            ('XGBOOST', xgboost_psuedo),\n",
    "            #('RF', rf_model),\n",
    "            ('GB', GB_psuedo),\n",
    "            ('CatBoost', CB_psuedo)]\n",
    "\n",
    "\n",
    "        # soft vote\n",
    "        soft_vote  = VotingClassifier(models, voting='soft')\n",
    "        model2 = soft_vote.fit(X_train, y_train)\n",
    "        model_list.append(model2)\n",
    "        \n",
    "        y_prob2 = np.round(model2.predict_proba(X_valid), 2)\n",
    "        y_prob2 = y_prob2[:, 1]\n",
    "        y_pred2 = np.where(y_prob2 > 0.5, 1, 0)\n",
    "        \n",
    "        metric2 = metrics.confusion_matrix(y_valid, y_pred2)\n",
    "        acc2 = metrics.accuracy_score(y_valid, y_pred2)\n",
    "        precision2 = metrics.precision_score(y_valid, y_pred2)\n",
    "        recall2 = metrics.recall_score(y_valid, y_pred2)\n",
    "        f12 = metrics.f1_score(y_valid, y_pred2)\n",
    "        auc2 = metrics.roc_auc_score(y_valid, y_prob2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # softvote\n",
    "        print('softvote accuracy: ', acc2)\n",
    "        print('softvote precision: ', precision2)\n",
    "        print('softvote recall: ', recall2)\n",
    "        print('softvote f1: ', f12)\n",
    "        print('softvote auc: ', auc2)\n",
    "        \n",
    "        acc_list2.append(acc2)\n",
    "        precision_list2.append(precision2)\n",
    "        recall_list2.append(recall2)\n",
    "        f1_list2.append(f12)\n",
    "        auc_list2.append(auc2)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    print('----------------------- final result ------------------------------')\n",
    "    print('softvote average of accuracy', np.mean(acc_list2))\n",
    "    print('softvote average of precision', np.mean(precision_list2))\n",
    "    print('softvote average of recall', np.mean(recall_list2))\n",
    "    print('softvote average of f1', np.mean(f1_list2))\n",
    "    print('softvote average of AUC', np.mean(auc_list2))\n",
    "    \n",
    "    for i in range(len(model_list)):\n",
    "        plot_roc(model_list[i], i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_list = []\n",
    "for model in model_list:\n",
    "    pred_y = model.predict_proba(X_te)\n",
    "    pred_y = pred_y[:,1]\n",
    "    pred_y_list.append(pred_y.reshape(-1,1))\n",
    "    \n",
    "mean_pred = np.mean(pred_y_list, axis=0)\n",
    "print(mean_pred)\n",
    "print(len(mean_pred))\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv')\n",
    "submission['problem'] = mean_pred.reshape(-1)\n",
    "submission.to_csv('ensemble_xgboost_rf_GB_CB_(psuedo)_0129.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(submission.problem)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "tot_1 = 0\n",
    "tot_0 = 0\n",
    "for i in submission.problem:\n",
    "    if i > 0.5:\n",
    "        tot_1 += 1\n",
    "    else:\n",
    "        tot_0 += 1\n",
    "\n",
    "print('0_class number: ', tot_0)\n",
    "print('1_class number: ', tot_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model2.predict_proba(X_te)\n",
    "pred_y = pred_y[:,1]\n",
    "    \n",
    "mean_pred = pred_y\n",
    "print(mean_pred)\n",
    "print(len(mean_pred))\n",
    "\n",
    "submission = pd.read_csv('./submission/sample_submission.csv')\n",
    "submission['problem'] = mean_pred.reshape(-1)\n",
    "submission.to_csv('kfx_ensemble_xgboost_rf_GB_CB_(psuedo)_0129.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(submission.problem)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "tot_1 = 0\n",
    "tot_0 = 0\n",
    "for i in submission.problem:\n",
    "    if i > 0.5:\n",
    "        tot_1 += 1\n",
    "    else:\n",
    "        tot_0 += 1\n",
    "\n",
    "print('0_class number: ', tot_0)\n",
    "print('1_class number: ', tot_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ROCAUC\n",
    " \n",
    "plt.figure(figsize=(12,12))\n",
    "visualizer = ROCAUC(model2, classes=['non_problem', 'problem'], micro=False, per_class=True)\n",
    "visualizer.fit(new_X, new_y)\n",
    "visualizer.score(X_valid, y_valid)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DT\n",
    "0.7831702070720021  \n",
    "{'max_depth': 9, 'min_samples_leaf': 35}  \n",
    "#### MLP\n",
    "0.7857024253649036  \n",
    "{'hidden_layer_sizes': 20, 'learning_rate_init': 0.01, 'max_iter': 1500, 'solver': 'adam'}  \n",
    "#### RF\n",
    "0.8118535984953457  \n",
    "{'max_depth': 14, min_samples_leaf': 30, 'min_samples_split': 30, 'n_estimators': 100}  \n",
    "#### SVM\n",
    "0.6744616049563809  \n",
    "{'C': 1, 'gamma': 0.001}  \n",
    "#### KNN\n",
    "0.6775991283087816\n",
    "{'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'distance'}  \n",
    "#### LGBM\n",
    "0.80084261997264\n",
    "{'boosting_type': 'gbdt', 'colsample_bytree': 0.64, 'learning_rate': 0.005,   \n",
    " 'max_bin': 255, 'n_estimators': 16, 'num_leaves': 16, 'objective': 'binary',   \n",
    " 'random_state': 500, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.7}  \n",
    " #### xgboost\n",
    "0.8167969043118297  \n",
    "{'booster': 'gbtree', 'gamma': 3, 'max_depth': 3, 'n_estimators': 30}\n",
    "#### gradient boosting\n",
    "0.8125282927485984  \n",
    "{'criterion': 'friedman_mse', 'learning_rate': 0.1, 'loss': 'exponential', 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 40}\n",
    "#### catboost\n",
    "0.8311812033338933  \n",
    "{'depth': 4, 'eval_metric': 'AUC', 'iterations': 500, 'l2_leaf_reg': 1e-20, 'leaf_estimation_iterations': 10, 'logging_level': 'Silent',   'loss_function': 'Logloss', 'random_seed': 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
